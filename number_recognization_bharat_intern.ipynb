{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1aab74",
   "metadata": {},
   "source": [
    "# TASK 1 GIVEN BY BHARAT INTERN INTERNSHIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809a380",
   "metadata": {},
   "source": [
    " HANDWRITTEN DIGIT RECOGNITION USING MNIST DATASET BY (PRACHI GOTHWAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2da652",
   "metadata": {},
   "source": [
    "The MNIST dataset is a widely used dataset in machine learning and consists of a set of 60,000 training images and 10,000 test images of handwritten digits from 0 to 9.Every time a data scientist or machine learning engineer makes a new algorithm for classification, they would always first check its performance on the MNIST dataset.It provides a baseline for testing image processing systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f363df5",
   "metadata": {},
   "source": [
    "########## FETCHING DATASET#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1214f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# numpy is aliased as np\n",
    "import pandas as pd\n",
    "# pandas is aliased as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# pyplot is aliased as plt\n",
    "import tensorflow as tf\n",
    "# tensorflow is aliased as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2347d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train), (x_test,y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a583dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "## checking the shapes of training and testing dataset\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428adc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "# displayiing the data of x_train\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1979624",
   "metadata": {},
   "source": [
    "Preprocess the data-->\n",
    "1.Reshaping the input data which is work as a input for CNN\n",
    "2.The original shape of x_train is (num_train_samples, 28, 28), where num_train_samples is the number of training samples,\n",
    "and 28 by 28 represents the dimensions of each image\n",
    "3.CNN takes the input data with the shape,(num_samples, height, width, channels)..\n",
    "4.Images are grayscale, so the channel value is set to 1\n",
    "5.After reshaping the datatype of array will change to float32 which is common data type used in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d03d4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b83f26",
   "metadata": {},
   "source": [
    "# NORMALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198958c",
   "metadata": {},
   "source": [
    "The original pixel values in the MNIST dataset range from 0 to 255, where 0 represents black and 255 represents white.\n",
    "Dividing the pixel values by 255 normalizes the data so that the values are scaled between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7266daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "x_train /= 340\n",
    "x_test /= 340\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cadab78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a179e4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779615e",
   "metadata": {},
   "source": [
    "# BUILDING OF NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "250e0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9df107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7744)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                77450     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 96,266\n",
      "Trainable params: 96,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3facb8bd",
   "metadata": {},
   "source": [
    "This summary shows that the model has five layers:\n",
    "\n",
    "Conv2D: This layer performs convolutional operations on the input. It has 32 filters of size 3x3 and applies the\n",
    "ReLU activation function. The output shape is (None, 26, 26, 32), where None represents the batch size.\n",
    "\n",
    "MaxPooling2D: This layer performs max pooling, which reduces the spatial dimensions of the input.\n",
    "It uses a 2x2 pooling window. The output shape is (None, 13, 13, 32), as the pooling operation reduces each spatial\n",
    "dimension by a factor of 2.\n",
    "\n",
    "Conv2D: This layer is similar to the previous Conv2D layer but has 64 filters instead of 32.\n",
    "The output shape is (None, 11, 11, 64).\n",
    "\n",
    "Flatten: This layer flattens the previous output, converting it from a 4D tensor to a 2D tensor.\n",
    "The output shape is (None, 7744), where 7744 is the result of multiplying the spatial dimensions.\n",
    "\n",
    "Dense: This layer is a fully connected layer with 10 units and applies the ReLU activation function.\n",
    "It takes the flattened input and produces an output of shape (None, 10), where 10 represents the number of classes in\n",
    "the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d587d",
   "metadata": {},
   "source": [
    "# PLOT NEURAL NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eca361a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\n",
    "#img = plt.imread('model.png')\n",
    "#plt.imshow(img)\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ada37e",
   "metadata": {},
   "source": [
    "# COMPILATION OF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "905d25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a7c20",
   "metadata": {},
   "source": [
    "# TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ff2a978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.5995 - accuracy: 0.8122 - val_loss: 0.3090 - val_accuracy: 0.9096\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.2875 - accuracy: 0.9160 - val_loss: 0.2232 - val_accuracy: 0.9356\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.1940 - accuracy: 0.9444 - val_loss: 0.1495 - val_accuracy: 0.9564\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1384 - accuracy: 0.9586 - val_loss: 0.1153 - val_accuracy: 0.9663\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1124 - accuracy: 0.9671 - val_loss: 0.0962 - val_accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2399d6c3c70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9864044",
   "metadata": {},
   "source": [
    "# ACCURACY TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03a5339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0962 - accuracy: 0.9705\n",
      "Test accuracy: 0.9704999923706055\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92ed43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAANpklEQVR4nO3df+hVdZ7H8ddrrQgcCy1Sc2p1p4jdijRFgoalJZzcCHSoSQ3KrYHvFFOMsH9szf4x0bIwxM7sP4LgVPgtXGMiXUWW1ZJB64+GvllbNu5Mbbj+zG8iNArhj3zvH9/j8h393nO/nXvuPVffzwd8ufee9/ec8/byfXnOvZ97z8cRIQAXvz9rugEAvUHYgSQIO5AEYQeSIOxAEpf0cme2eesf6LKI8FjLOzqy215o+/e2P7X9dCfbAtBdrjrObnuCpD9IWiBpv6R3JS2LiN+VrMORHeiybhzZ50v6NCI+i4iTkl6VtKiD7QHook7CPkPSvlGP9xfL/oTtAdtDtoc62BeADnXyBt1YpwrnnaZHxGpJqyVO44EmdXJk3y/pulGPvy3pYGftAOiWTsL+rqQbbc+yfZmkpZI21dMWgLpVPo2PiNO2n5S0RdIESS9FxMe1dQagVpWH3irtjNfsQNd15UM1AC4chB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRRecpm1GfevHml9aGhoR51cmF54IEHSuvbt29vWfviiy/qbqfvdRR223skHZP0taTTEVH+VwugMXUc2f8mIo7UsB0AXcRrdiCJTsMekrbafs/2wFi/YHvA9pBtXngCDer0NP7OiDho+xpJb9j+74jYMfoXImK1pNWSZDs63B+Aijo6skfEweJ2WNIGSfPraApA/SqH3fZE25PO3pf0PUm76moMQL06OY2fKmmD7bPb+beI+M9aurrI3HPPPaX1tWvXltaHh4dL67fddlvL2qlTp0rX7WdLly4trQ8ODpbWN27c2LL24IMPVurpQlY57BHxmaTWf2UA+gpDb0AShB1IgrADSRB2IAnCDiTBV1xrsHDhwtL6K6+8UlqfMmVKR/Vi+POic+LEidL6mTNnSusLFixoWXv55ZdL133kkUdK6xcijuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7OM0ceLElrXnnnuudN2rrrqqtN5uvHjVqlWl9dOnT5fWL1QbNmworR88eLC0PmvWrJa1O+64o3TdSZMmldaPHTtWWu9HHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2cdp5cqVLWvtplxup913q5966qmOto/z3XDDDaX1+++/v7S+Zs2aGrvpDY7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+yF22+/vbR+3333Vd720aNHS+tlY/hAXdoe2W2/ZHvY9q5Ry6bYfsP2J8Xt5O62CaBT4zmNXyPp3ClPnpa0LSJulLSteAygj7UNe0TskHTueegiSYPF/UFJi+ttC0Ddqr5mnxoRhyQpIg7ZvqbVL9oekDRQcT8AatL1N+giYrWk1ZJkO7q9PwBjqzr0dtj2dEkqbofrawlAN1QN+yZJy4v7yyVtrKcdAN3iiPIza9vrJN0l6WpJhyX9TNK/S/q1pOsl7ZX0g4goH0xWs6fxc+fOLa1v27attH7FFVdU3veSJUtK66+99lrlbWc2ODhYWn/44Ycrb/vzzz8vrV977bWVt91tEeGxlrd9zR4Ry1qU7u6oIwA9xcdlgSQIO5AEYQeSIOxAEoQdSCLNV1zvvrt88KCTobX333+/tL558+bK20Zrjz/+eGn91ltvbVmbPXt26boTJkyo0lJf48gOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mkGWdfsWJFR+sfOXKkZe2ZZ54pXferr77qaN8YW7vn9eTJk5W3fdlll5XWZ86cWVrfs2dP5X13C0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUgizTj79OnTS+vtLqn9zjvvtKxt3bq1Uk/ZTZs2raN6OxMnTqy87pVXXlla37JlS2n9pptuqrzvbuHIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJtJ2yudadNThlc7t/Z7v6gQMHWtbWrl1buu7KlStL6900Z86c0vqyZa0m6e2+m2++ubRedt33pn355Zel9cmTJ/eok/O1mrK57ZHd9ku2h23vGrXsWdsHbH9Q/NxbZ7MA6jee0/g1khaOsfxfI2J28fMf9bYFoG5twx4ROyQd7UEvALqokzfonrT9YXGa3/IFiu0B20O2hzrYF4AOVQ37KknfkTRb0iFJv2j1ixGxOiLmRcS8ivsCUINKYY+IwxHxdUSckfQrSfPrbQtA3SqF3fbo74t+X9KuVr8LoD+0HWe3vU7SXZKulnRY0s+Kx7MlhaQ9kn4UEYfa7qzBcfb169eX1hcvXtybRnBBOH78eGn9rrvuKq3v3Lmzxm6+mVbj7G0vXhERY33q4sWOOwLQU3xcFkiCsANJEHYgCcIOJEHYgSTSfMW1nXXr1pXWlyxZ0qNO8njrrbdK69u3b+9o+48++mjL2owZM0rXffXVV0vrDz30UKWeeqHyV1wBXBwIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtkLl156aWn98ssvb1l74oknStedNWtWpZ7G6+23325Z27dvX+m6c+fOLa2/8MILlXoaj5MnT5bWT5w40dH2y6bZnj+//Horjz32WGl9zZo1VVrqCcbZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJtleXzeLUqVOV688//3zd7fTMjh07mm6hsltuuaW0fv311/eokwsDR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJxdlywdu3aVVrfu3dvy9q0adPqbqfvtT2y277O9m9s77b9se2fFMun2H7D9ifF7eTutwugqvGcxp+W9PcR8ZeS7pD0Y9t/JelpSdsi4kZJ24rHAPpU27BHxKGI2FncPyZpt6QZkhZJGix+bVDS4i71CKAG3+g1u+2ZkuZI+q2kqRFxSBr5D8H2NS3WGZA00GGfADo07rDb/pak1yWtiIg/2mNe0+48EbFa0upiG317wUngYjeuoTfbl2ok6GsjYn2x+LDt6UV9uqTh7rQIoA5tj+weOYS/KGl3RPxyVGmTpOWSfl7cbuxKh0BFw8Otjz9DQ0Ol67755pt1t9O48ZzG3ynpYUkf2f6gWPZTjYT817Z/KGmvpB90pUMAtWgb9oh4W1KrF+h319sOgG7h47JAEoQdSIKwA0kQdiAJwg4kwZTNuGiVfY31kkvKB6L2799fdzs9w5TNQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE4+zARYZxdiA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiibdhtX2f7N7Z32/7Y9k+K5c/aPmD7g+Ln3u63C6CqthevsD1d0vSI2Gl7kqT3JC2W9KCk4xHxL+PeGRevALqu1cUrxjM/+yFJh4r7x2zvljSj3vYAdNs3es1ue6akOZJ+Wyx60vaHtl+yPbnFOgO2h2wPddYqgE6M+xp0tr8labukf46I9banSjoiKST9k0ZO9R9rsw1O44Eua3UaP66w275U0mZJWyLil2PUZ0raHBG3tNkOYQe6rPIFJ21b0ouSdo8OevHG3Vnfl7Sr0yYBdM943o3/rqS3JH0k6Uyx+KeSlkmarZHT+D2SflS8mVe2LY7sQJd1dBpfF8IOdB/XjQeSI+xAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTR9oKTNTsi6X9HPb66WNaP+rW3fu1Loreq6uztz1sVevp99vN2bg9FxLzGGijRr731a18SvVXVq944jQeSIOxAEk2HfXXD+y/Tr731a18SvVXVk94afc0OoHeaPrID6BHCDiTRSNhtL7T9e9uf2n66iR5asb3H9kfFNNSNzk9XzKE3bHvXqGVTbL9h+5Pidsw59hrqrS+m8S6ZZrzR567p6c97/prd9gRJf5C0QNJ+Se9KWhYRv+tpIy3Y3iNpXkQ0/gEM238t6bikl89OrWX7eUlHI+LnxX+UkyPiH/qkt2f1Dafx7lJvraYZ/zs1+NzVOf15FU0c2edL+jQiPouIk5JelbSogT76XkTskHT0nMWLJA0W9wc18sfScy166wsRcSgidhb3j0k6O814o89dSV890UTYZ0jaN+rxfvXXfO8haavt92wPNN3MGKaenWaruL2m4X7O1XYa7146Z5rxvnnuqkx/3qkmwj7W1DT9NP53Z0TcLulvJf24OF3F+KyS9B2NzAF4SNIvmmymmGb8dUkrIuKPTfYy2hh99eR5ayLs+yVdN+rxtyUdbKCPMUXEweJ2WNIGjbzs6CeHz86gW9wON9zP/4uIwxHxdUSckfQrNfjcFdOMvy5pbUSsLxY3/tyN1Vevnrcmwv6upBttz7J9maSlkjY10Md5bE8s3jiR7YmSvqf+m4p6k6Tlxf3lkjY22Muf6JdpvFtNM66Gn7vGpz+PiJ7/SLpXI+/I/4+kf2yihxZ9/YWk/yp+Pm66N0nrNHJad0ojZ0Q/lHSVpG2SPilup/RRb69oZGrvDzUSrOkN9fZdjbw0/FDSB8XPvU0/dyV99eR54+OyQBJ8gg5IgrADSRB2IAnCDiRB2IEkCDuQBGEHkvg/vDI5u7VQwKIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = x_train[60]\n",
    "plt.imshow(np.squeeze(img) ,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e456f5",
   "metadata": {},
   "source": [
    "# RESULT AFTER PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a5d47e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n",
      "Predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "img= img.reshape(1, 28, 28, 1)\n",
    "a= model.predict([img])\n",
    "predicted_class = np.argmax(a)\n",
    "print(\"Predicted class: {}\".format(predicted_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
